{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6837fc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f16cf10a-cd54-4625-ab08-7062d5e97905",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m AsyncClient()\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m     11\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3\u001b[39m\u001b[38;5;124m\"\u001b[39m, messages\u001b[38;5;241m=\u001b[39m[message], stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     ):\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(part[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m], end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "#test ollama\n",
    "\n",
    "async def chat():\n",
    "    \"\"\"\n",
    "    Stream a chat from Llama using the AsyncClient.\n",
    "    \"\"\"\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me an interesting fact about elephants\"\n",
    "    }\n",
    "    async for part in await AsyncClient().chat(\n",
    "        model=\"llama3\", messages=[message], stream=True\n",
    "    ):\n",
    "        print(part[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "\n",
    "\n",
    "asyncio.run(chat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d76295-6100-430f-90a5-1b9225404203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chatbot_response(client, mode_name, messages, temperature=0):\n",
    "    input_messages = []\n",
    "    for message in messages:\n",
    "        input_messages.append({\"role\": message[\"role\"], \"content\": message[\"content\"]})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=input_messages,\n",
    "        temperature=temperature,\n",
    "        top_p=0.8,\n",
    "        max_tokens=2000\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31bf941-a233-4663-8f5b-8597b208d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"RUN_POD_TOKEN\"),\n",
    "    base_url=os.getenv(\"RUN_POD_BASE_URL\")\n",
    ")\n",
    "model_name= os.getenv(\"RUN_POD_MODEL_NAME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ab269-eaca-47d6-ac41-273a294c40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\":\"system\", \"content\": \"What is the capital of Germany\"}]\n",
    "response = get_chatbot_response(client, model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc2ac8-04f7-4615-b51d-2a8b2b8e9d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2396ed05-7de9-468a-9a3f-69ac0532c429",
   "metadata": {},
   "source": [
    "## Prompt Engineering \n",
    "### Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e92f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    " You are a helpful assistant that answers questions about capitals of countries\n",
    "\n",
    " Your output should be in a structured format like the one below. You are not allowed to write anything other than the json object:\n",
    "\n",
    "[{\n",
    "    \"country\": the country that you will give the capital of,\n",
    "    \"capital\": the capital of the country stated}]\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "messages.append({\"role\": \"user\", \"content\": \"What is the capital of Spain\"})\n",
    "response = get_chatbot_response(client, model_name, messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the response to a json object\n",
    "json_response = json.loads(response)\n",
    "json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348e0dc",
   "metadata": {},
   "source": [
    "## Input Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa008425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27692d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inout = \"\"\"\n",
    "Get me the capital fo the following countries\n",
    "```\n",
    "1. Kenya\n",
    "2. Ukraine\n",
    "3. Spain\n",
    "4. Poland\n",
    "4. Germany\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "messages.append({\"role\": \"user\", \"content\": user_inout})\n",
    "response = get_chatbot_response(client, model_name, messages)\n",
    "\n",
    "## convert the response to a json object\n",
    "json_response = json.loads(response)\n",
    "json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd2a86",
   "metadata": {},
   "source": [
    "### Give the model time to think - (Chain of thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "Calculate the result of this eqaution: 1 + 3\n",
    "\n",
    "Your output should be in a structured json format exactly like the one below. You are not allowed to write anything other than the json object:\n",
    "{\n",
    " result: The final number resulted from the calculation of the equation above\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "response = get_chatbot_response(client, model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f586b38",
   "metadata": {},
   "source": [
    "### Decrease the margin of error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "Calculate the result of this eqaution: 259 + 341 - 100 * 20008 / 0.5 + 27683 / 2 * 365\n",
    "\n",
    "Your output should be in a structured json format exactly like the one below. You are not allowed to write anything other than the json object:\n",
    "{\n",
    " steps: This is where you solve the equation bit by bit following the BEDMAS rule of order of operations. You need to show your work and calculate each step leading into the final results on feel free to write in free text\n",
    " result: The final number resulted from the calculation of the equation above\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "response = get_chatbot_response(client, model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c76a1ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59722ca8",
   "metadata": {},
   "source": [
    "### RAG - (Retrival Augmented Generation) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadda9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "\n",
    "What is new in iphone 16?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "response = get_chatbot_response(client, model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone_16 = \"\"\"\n",
    "The iPhone 16 introduces several exciting updates, making it one of Apple's most advanced smartphones to date. It features a larger 6.1-inch display for the base model and a 6.7-inch screen for the iPhone 16 Plus, with thinner bezels and a more durable Ceramic Shield. The iPhone 16 Pro and Pro Max boast even larger displays, measuring 6.3 and 6.9 inches respectively, offering the thinnest bezels seen on any Apple product so far.\n",
    "\n",
    "Powered by the new A18 chip (A18 Pro for the Pro models), these phones deliver significant performance improvements, with enhanced neural engine capabilities, faster GPU for gaming, and machine learning tasks. The camera systems are also upgraded, with the base iPhone 16 sporting a dual-camera setup with a 48MP main sensor. The Pro models offer a 48MP Ultra Wide and 5x telephoto camera, enhanced by Appleâ€™s \"Camera Control\" button for more flexible photography options.\n",
    "\n",
    "Apple also introduced advanced audio features like \"Audio Mix,\" which uses machine learning to separate background sounds from speech, allowing for more refined audio capture during video recording. Battery life has been extended, especially in the iPhone 16 Pro Max, which is claimed to have the longest-lasting battery of any iPhone \n",
    "9TO5MAC\n",
    "\n",
    "APPLEMAGAZINE\n",
    ".\n",
    "\n",
    "Additionally, Apple has switched to USB-C for faster charging and data transfer, and the Pro models now support up to 2x faster video encoding. The starting prices remain consistent with previous generations, with the iPhone 16 starting at $799, while the Pro models start at $999\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"  \n",
    "{iphone_16}\n",
    "\n",
    "What is new in iphone 16?\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "response = get_chatbot_response(client, model_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778d6a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f415f5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292dfd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f097e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a32e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36ffbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b251d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff87dc41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad10b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e8f066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf54fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
